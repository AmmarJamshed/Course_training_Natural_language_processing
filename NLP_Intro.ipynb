{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTge/Y2jn+IO+I2oIeFIdt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmmarJamshed/Course_training_Natural_language_processing/blob/main/NLP_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOkenization\n",
        "- the process of converting a sequence of text into smaller parts, known as tokens."
      ],
      "metadata": {
        "id": "2KFy-tHlD9_H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5z7ECfXD0eR",
        "outputId": "d68331ce-f512-4550-9ea8-1b0271417c05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', 'is', 'fun', 'and', 'challenging', '!']\n"
          ]
        }
      ],
      "source": [
        "    import nltk\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    nltk.download('punkt')\n",
        "\n",
        "    text = \"Natural language processing is fun and challenging!\"\n",
        "    tokens = word_tokenize(text)\n",
        "    print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition (NER) and Part-of-Speech Tagging (POS)\n",
        "- With POS tagging, we can identify the different parts of speech in a sentence, which can be useful for tasks like sentiment analysis and text classification. Meanwhile, NER helps us to identify and classify named entities in text, such as people, organizations, locations, and dates"
      ],
      "metadata": {
        "id": "RHMGbx9OEfNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6elWxz60D839",
        "outputId": "c3c95311-43fc-453e-d3b1-9abf27f8def5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "U.K. GPE\n",
            "$1 billion MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL Creation of NLP"
      ],
      "metadata": {
        "id": "1J_8wvoYHaql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Text Representation Techniques\n",
        "- It aims representing each text document by a numerical vector such that the similarity between vectors (documents) can be computed by different kernels."
      ],
      "metadata": {
        "id": "9NBp5v-2E-lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = [[\"word\", \"embeddings\", ...], [\"another\", \"sentence\", ...]]\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)"
      ],
      "metadata": {
        "id": "pY25KCQFEwex"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gtui1BkkHWRG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}